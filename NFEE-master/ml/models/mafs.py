from itertools import izip

import numpy as np
import theano
import theano.tensor as tt

import ml.models.layers as layers
import ml.models.mades as mades

import util.misc
import util.ml

dtype = theano.config.floatX


class MaskedAutoregressiveFlow:
    """
    Implements a Masked Autoregressive Flow, which is a stack of mades such that the random numbers which drive made i
    are generated by made i-1. The first made is driven by standard gaussian noise. In the current implementation, all
    mades are of the same type. If there is only one made in the stack, then it's equivalent to a single made.
    """

    def __init__(self, n_inputs, n_hiddens, act_fun, n_mades, batch_norm=True, input_order='sequential', mode='sequential', input=None, rng=np.random):
        """
        Constructor.
        :param n_inputs: number of inputs
        :param n_hiddens: list with number of hidden units for each hidden layer
        :param act_fun: name of activation function
        :param n_mades: number of mades
        :param batch_norm: whether to use batch normalization between mades
        :param input_order: order of inputs of last made
        :param mode: strategy for assigning degrees to hidden nodes: can be 'random' or 'sequential'
        :param input: theano variable to serve as input; if None, a new variable is created
        """

        # save input arguments
        self.n_inputs = n_inputs
        self.n_hiddens = n_hiddens
        self.act_fun = act_fun
        self.n_mades = n_mades
        self.batch_norm = batch_norm
        self.mode = mode

        self.input = tt.matrix('x', dtype=dtype) if input is None else input
        self.parms = []

        self.mades = []
        self.bns = []
        self.u = self.input
        self.logdet_dudx = 0.0

        for i in xrange(n_mades):

            # create a new made
            made = mades.GaussianMade(n_inputs, n_hiddens, act_fun, input_order, mode, self.u, rng)
            self.mades.append(made)
            self.parms += made.parms
            input_order = input_order if input_order == 'random' else made.input_order[::-1]

            # inverse autoregressive transform
            self.u = made.u
            self.logdet_dudx += 0.5 * tt.sum(made.logp, axis=1)

            # batch normalization
            if batch_norm:
                bn = layers.BatchNorm(self.u, n_inputs)
                self.u = bn.y
                self.parms += bn.parms
                self.logdet_dudx += tt.sum(bn.log_gamma) - 0.5 * tt.sum(tt.log(bn.v))
                self.bns.append(bn)

        self.input_order = self.mades[0].input_order

        # log likelihoods
        self.L = -0.5 * n_inputs * np.log(2 * np.pi) - 0.5 * tt.sum(self.u ** 2, axis=1) + self.logdet_dudx
        self.L.name = 'L'

        # train objective
        self.trn_loss = -tt.mean(self.L)
        self.trn_loss.name = 'trn_loss'

        # theano evaluation functions, will be compiled when first needed
        self.eval_lprob_f = None
        self.eval_grad_f = None
        self.eval_us_f = None

    def reset_theano_functions(self):
        """
        Resets theano functions, so that they are compiled again when needed.
        """

        self.eval_lprob_f = None
        self.eval_grad_f = None
        self.eval_us_f = None

        for made in self.mades:
            made.reset_theano_functions()

        for bn in self.bns:
            bn.reset_theano_functions()

    def eval(self, x, log=True):
        """
        Evaluate log probabilities for given inputs.
        :param x: data matrix where rows are inputs
        :param log: whether to return probabilities in the log domain
        :return: list of log probabilities log p(x)
        """

        # compile theano function, if haven't already done so
        if self.eval_lprob_f is None:
            self.eval_lprob_f = theano.function(
                inputs=[self.input],
                outputs=self.L,
                givens=[(bn.m, bn.bm) for bn in self.bns] + [(bn.v, bn.bv) for bn in self.bns]
            )

        x = np.asarray(x, dtype=dtype)
        lprob = self.eval_lprob_f(x[np.newaxis, :])[0] if x.ndim == 1 else self.eval_lprob_f(x)

        return lprob if log else np.exp(lprob)

    def grad_log_p(self, x):
        """
        Evaluate the gradient of the log probability wrt the input.
        :param x: rows are input locations
        :return: gradient d/dx log p(x)
        """

        # compile theano function, if haven't already done so
        if getattr(self, 'eval_grad_f', None) is None:
            self.eval_grad_f = theano.function(
                inputs=[self.input],
                outputs=tt.grad(tt.sum(self.L), self.input),
                givens=[(bn.m, bn.bm) for bn in self.bns] + [(bn.v, bn.bv) for bn in self.bns]
            )

        x = np.asarray(x, dtype=dtype)
        grad = self.eval_grad_f(x[np.newaxis, :])[0] if x.ndim == 1 else self.eval_grad_f(x)

        return grad

    def gen(self, n_samples=None, u=None, rng=np.random):
        """
        Generate samples, by propagating random numbers through each made.
        :param n_samples: number of samples, 1 if None
        :param u: random numbers to use in generating samples; if None, new random numbers are drawn
        :return: samples
        """

        if n_samples is None:
            return self.gen(1, u if u is None else u[np.newaxis, :], rng)[0]

        x = rng.randn(n_samples, self.n_inputs).astype(dtype) if u is None else u

        if getattr(self, 'batch_norm', False):

            for made, bn in izip(self.mades[::-1], self.bns[::-1]):
                x = bn.eval_inv(x)
                x = made.gen(n_samples, x, rng)

        else:

            for made in self.mades[::-1]:
                x = made.gen(n_samples, x, rng)

        return x

    def calc_random_numbers(self, x):
        """
        Givan a dataset, calculate the random numbers used internally to generate the dataset.
        :param x: numpy array, rows are datapoints
        :return: numpy array, rows are corresponding random numbers
        """

        # compile theano function, if haven't already done so
        if self.eval_us_f is None:
            self.eval_us_f = theano.function(
                inputs=[self.input],
                outputs=self.u
            )

        x = np.asarray(x, dtype=dtype)

        return self.eval_us_f(x[np.newaxis, :])[0] if x.ndim == 1 else self.eval_us_f(x)
    
    def logdet_jacobi_u(self, x):
        
        if getattr(self, 'eval_jacobian_u', None) is None:
            self.eval_jacobian_u = theano.function(
                    inputs=[self.input],
                    outputs=self.logdet_dudx
                    )
    
        x = np.asarray(x, dtype=dtype)
        logdet_jacobi = self.eval_jacobian_u(x[np.newaxis, :])[0] if x.ndim == 1 else self.eval_jacobian_u(x)

        return logdet_jacobi
        


class ConditionalMaskedAutoregressiveFlow:
    """
    Implements a Conditional Masked Autoregressive Flow.
    """

    def __init__(self, n_inputs, n_outputs, n_hiddens, act_fun, n_mades, batch_norm=True, output_order='sequential', mode='sequential', input=None, output=None, rng=np.random):
        """
        Constructor.
        :param n_inputs: number of (conditional) inputs
        :param n_outputs: number of outputs
        :param n_hiddens: list with number of hidden units for each hidden layer
        :param act_fun: name of activation function
        :param n_mades: number of mades in the flow
        :param batch_norm: whether to use batch normalization between mades in the flow
        :param output_order: order of outputs of last made
        :param mode: strategy for assigning degrees to hidden nodes: can be 'random' or 'sequential'
        :param input: theano variable to serve as input; if None, a new variable is created
        :param output: theano variable to serve as output; if None, a new variable is created
        """

        # save input arguments
        self.n_inputs = n_inputs
        self.n_outputs = n_outputs
        self.n_hiddens = n_hiddens
        self.act_fun = act_fun
        self.n_mades = n_mades
        self.batch_norm = batch_norm
        self.mode = mode

        self.input = tt.matrix('x', dtype=dtype) if input is None else input
        self.y = tt.matrix('y', dtype=dtype) if output is None else output
        self.parms = []

        self.mades = []
        self.bns = []
        self.u = self.y
        self.logdet_dudy = 0.0

        for i in xrange(n_mades):

            # create a new made
            made = mades.ConditionalGaussianMade(n_inputs, n_outputs, n_hiddens, act_fun, output_order, mode, self.input, self.u, rng)
            self.mades.append(made)
            self.parms += made.parms
            output_order = output_order if output_order == 'random' else made.output_order[::-1]

            # inverse autoregressive transform
            self.u = made.u
            self.logdet_dudy += 0.5 * tt.sum(made.logp, axis=1)

            # batch normalization
            if batch_norm:
                bn = layers.BatchNorm(self.u, n_outputs)
                self.u = bn.y
                self.parms += bn.parms
                self.logdet_dudy += tt.sum(bn.log_gamma) - 0.5 * tt.sum(tt.log(bn.v))
                self.bns.append(bn)

        self.output_order = self.mades[0].output_order

        # log likelihoods
        self.L = -0.5 * n_outputs * np.log(2 * np.pi) - 0.5 * tt.sum(self.u ** 2, axis=1) + self.logdet_dudy
        self.L.name = 'L'

        # train objective
        self.trn_loss = -tt.mean(self.L)
        self.trn_loss.name = 'trn_loss'

        # theano evaluation functions, will be compiled when first needed
        self.eval_lprob_f = None
        self.eval_grad_f = None
        self.eval_score_f = None
        self.eval_us_f = None

    def reset_theano_functions(self):
        """
        Resets theano functions, so that they are compiled again when needed.
        """

        self.eval_lprob_f = None
        self.eval_grad_f = None
        self.eval_score_f = None
        self.eval_us_f = None

        for made in self.mades:
            made.reset_theano_functions()

        for bn in self.bns:
            bn.reset_theano_functions()

    def eval(self, xy, log=True):
        """
        Evaluate log probabilities for given input-output pairs.
        :param xy: a pair (x, y) where x rows are inputs and y rows are outputs
        :param log: whether to return probabilities in the log domain
        :return: log probabilities: log p(y|x)
        """

        # compile theano function, if haven't already done so
        if self.eval_lprob_f is None:
            self.eval_lprob_f = theano.function(
                inputs=[self.input, self.y],
                outputs=self.L,
                givens=[(bn.m, bn.bm) for bn in self.bns] + [(bn.v, bn.bv) for bn in self.bns]
            )

        x, y, one_datapoint = util.misc.prepare_cond_input(xy, dtype)

        lprob = self.eval_lprob_f(x, y)
        lprob = lprob[0] if one_datapoint else lprob

        return lprob if log else np.exp(lprob)

    def grad_log_p(self, xy):
        """
        Evaluate the gradient of the log probability wrt the output, for given input-output pairs.
        :param xy: a pair (x, y) where x rows are inputs and y rows are outputs
        :return: gradient d/dy log p(y|x)
        """

        # compile theano function, if haven't already done so
        if getattr(self, 'eval_grad_f', None) is None:
            self.eval_grad_f = theano.function(
                inputs=[self.input, self.y],
                outputs=tt.grad(tt.sum(self.L), self.y),
                givens=[(bn.m, bn.bm) for bn in self.bns] + [(bn.v, bn.bv) for bn in self.bns]
            )

        x, y, one_datapoint = util.misc.prepare_cond_input(xy, dtype)

        grad = self.eval_grad_f(x, y)
        grad = grad[0] if one_datapoint else grad

        return grad

    def score(self, xy):
        """
        Evaluate the gradient of the log probability wrt the input, for given input-output pairs.
        :param xy: a pair (x, y) where x rows are inputs and y rows are outputs
        :return: gradient d/dx log p(y|x)
        """

        # compile theano function, if haven't already done so
        if self.eval_score_f is None:
            self.eval_score_f = theano.function(
                inputs=[self.input, self.y],
                outputs=tt.grad(tt.sum(self.L), self.input),
                givens=[(bn.m, bn.bm) for bn in self.bns] + [(bn.v, bn.bv) for bn in self.bns]
            )

        x, y, one_datapoint = util.misc.prepare_cond_input(xy, dtype)

        grads = self.eval_score_f(x, y)
        grads = grads[0] if one_datapoint else grads

        return grads

    def gen(self, x, n_samples=None, u=None, rng=np.random):
        """
        Generate samples, by propagating random numbers through each made, after conditioning on input x.
        :param x: input vector
        :param n_samples: number of samples, 1 if None
        :param u: random numbers to use in generating samples; if None, new random numbers are drawn
        :return: samples
        """

        if n_samples is None:
            return self.gen(x, 1, u if u is None else u[np.newaxis, :], rng)[0]

        y = rng.randn(n_samples, self.n_outputs).astype(dtype) if u is None else u

        if getattr(self, 'batch_norm', False):

            for made, bn in izip(self.mades[::-1], self.bns[::-1]):
                y = bn.eval_inv(y)
                y = made.gen(x, n_samples, y, rng)

        else:

            for made in self.mades[::-1]:
                y = made.gen(x, n_samples, y, rng)

        return y

    def calc_random_numbers(self, xy):
        """
        Given a dataset, calculate the random numbers used internally to generate the dataset.
        :param xy: a pair (x, y) of numpy arrays, where x rows are inputs and y rows are outputs
        :return: numpy array, rows are corresponding random numbers
        """

        # compile theano function, if haven't already done so
        if self.eval_us_f is None:
            self.eval_us_f = theano.function(
                inputs=[self.input, self.y],
                outputs=self.u
            )

        x, y, one_datapoint = util.misc.prepare_cond_input(xy, dtype)
        u = self.eval_us_f(x, y)

        return u[0] if one_datapoint else u

    def logdet_jacobi_u(self, xy):
        """
        Evaluate the gradient of the log probability wrt the output, for given input-output pairs.
        :param xy: a pair (x, y) where x rows are inputs and y rows are outputs
        :return: gradient d/dy log p(y|x)
        """

        # compile theano function, if haven't already done so
        if getattr(self, 'eval_jacobi_u', None) is None:
            self.eval_jacobian_u = theano.function(
                inputs=[self.input, self.y],
                outputs=self.logdet_dudy
                )

        x, y, one_datapoint = util.misc.prepare_cond_input(xy, dtype)

        logdet_jacobi = self.eval_jacobian_u(x, y)
        logdet_jacobi = logdet_jacobi[0] if one_datapoint else logdet_jacobi

        return logdet_jacobi


class MaskedAutoregressiveFlow_on_MADE:
    """
    A Masked Autoregressive Flow, where the target distribution is given by a MoG MADE.
    """

    def __init__(self, n_inputs, n_hiddens, act_fun, n_layers, n_comps, batch_norm=True, input_order='sequential', mode='sequential', input=None, rng=np.random):
        """
        Constructor.
        :param n_inputs: number of inputs
        :param n_hiddens: list with number of hidden units for each hidden layer
        :param act_fun: name of activation function
        :param n_layers: number of layers in the flow
        :param n_comps: number of gaussians per conditional for the target made
        :param batch_norm: whether to use batch normalization between layers
        :param input_order: order of inputs of last made
        :param mode: strategy for assigning degrees to hidden nodes: can be 'random' or 'sequential'
        :param input: theano variable to serve as input; if None, a new variable is created
        """

        # save input arguments
        self.n_inputs = n_inputs
        self.n_hiddens = n_hiddens
        self.act_fun = act_fun
        self.n_layers = n_layers
        self.n_comps = n_comps
        self.batch_norm = batch_norm
        self.mode = mode

        self.input = tt.matrix('x', dtype=dtype) if input is None else input
        self.parms = []

        # maf
        self.maf = MaskedAutoregressiveFlow(n_inputs, n_hiddens, act_fun, n_layers, batch_norm, input_order, mode, self.input, rng)
        self.bns = self.maf.bns
        self.parms += self.maf.parms
        self.input_order = self.maf.input_order

        # mog made
        input_order = input_order if input_order == 'random' else self.maf.mades[-1].input_order[::-1]
        self.made = mades.MixtureOfGaussiansMade(n_inputs, n_hiddens, act_fun, n_comps, input_order, mode, self.maf.u, rng)
        self.parms += self.made.parms

        # log likelihoods
        self.L = self.made.L + self.maf.logdet_dudx
        self.L.name = 'L'

        # train objective
        self.trn_loss = -tt.mean(self.L)
        self.trn_loss.name = 'trn_loss'

        # theano evaluation functions, will be compiled when first needed
        self.eval_lprob_f = None
        self.eval_grad_f = None
        self.eval_us_f = None

    def reset_theano_functions(self):
        """
        Resets theano functions, so that they are compiled again when needed.
        """

        self.eval_lprob_f = None
        self.eval_grad_f = None
        self.eval_us_f = None

        self.maf.reset_theano_functions()
        self.made.reset_theano_functions()

        for bn in self.bns:
            bn.reset_theano_functions()

    def eval(self, x, log=True):
        """
        Evaluate log probabilities for given inputs.
        :param x: data matrix where rows are inputs
        :param log: whether to return probabilities in the log domain
        :return: list of log probabilities log p(x)
        """

        # compile theano function, if haven't already done so
        if self.eval_lprob_f is None:
            self.eval_lprob_f = theano.function(
                inputs=[self.input],
                outputs=self.L,
                givens=[(bn.m, bn.bm) for bn in self.bns] + [(bn.v, bn.bv) for bn in self.bns]
            )

        x = np.array(x, dtype=dtype)
        lprob = self.eval_lprob_f(x[np.newaxis, :])[0] if x.ndim == 1 else self.eval_lprob_f(x)

        return lprob if log else np.exp(lprob)

    def grad_log_p(self, x):
        """
        Evaluate the gradient of the log probability wrt the input.
        :param x: rows are input locations
        :return: gradient d/dx log p(x)
        """

        # compile theano function, if haven't already done so
        if getattr(self, 'eval_grad_f', None) is None:
            self.eval_grad_f = theano.function(
                inputs=[self.input],
                outputs=tt.grad(tt.sum(self.L), self.input),
                givens=[(bn.m, bn.bm) for bn in self.bns] + [(bn.v, bn.bv) for bn in self.bns]
            )

        x = np.asarray(x, dtype=dtype)
        grad = self.eval_grad_f(x[np.newaxis, :])[0] if x.ndim == 1 else self.eval_grad_f(x)

        return grad

    def gen(self, n_samples=None, u=None, rng=np.random):
        """
        Generate samples, by propagating random numbers through each made.
        :param n_samples: number of samples, 1 if None
        :param u: random numbers to use in generating samples; if None, new random numbers are drawn
        :return: samples
        """

        if n_samples is None:
            return self.gen(1, u if u is None else u[np.newaxis, :], rng)[0]

        x = rng.randn(n_samples, self.n_inputs).astype(dtype) if u is None else u
        x = self.made.gen(n_samples, x, rng)
        x = self.maf.gen(n_samples, x, rng)

        return x

    def calc_random_numbers(self, x):
        """
        Givan a dataset, calculate the random numbers used internally to generate the dataset.
        :param x: numpy array, rows are datapoints
        :return: numpy array, rows are corresponding random numbers
        """

        # compile theano function, if haven't already done so
        if self.eval_us_f is None:
            self.eval_us_f = theano.function(
                inputs=[self.input],
                outputs=self.made.u
            )

        x = np.array(x, dtype=dtype)

        return self.eval_us_f(x[np.newaxis, :])[0] if x.ndim == 1 else self.eval_us_f(x)

    def jacobian_u(self, x):
        
        if getattr(self, 'batch_norm', True):
            raise ValueError('jacobian_u do not support batch_norm')
        
        #J,_ = theano.scan(fn=lambda u, x : theano.gradient.jacobian(u, x), sequences=[self.u, self.input])
        if getattr(self, 'eval_jacobian_u', None) is None:
            self.eval_jacobian_u = theano.function(
                    inputs=[self.input],
                    outputs=theano.gradient.jacobian(self.u[0], self.input)
                    )
    
        x = np.asarray(x, dtype=dtype)
        assert x.ndim == 1, 'only support a row of x as input'
        
        jacobi = self.eval_jacobian_u(x[np.newaxis, :])
        
        return jacobi[:,0,:]
    

class ConditionalMaskedAutoregressiveFlow_on_MADE:
    """
    Conditional Masked Autoregressive Flow, where the target distribution is a conditional Mog MADE.
    """

    def __init__(self, n_inputs, n_outputs, n_hiddens, act_fun, n_layers, n_comps, batch_norm=True, output_order='sequential', mode='sequential', input=None, output=None, rng=np.random):
        """
        Constructor.
        :param n_inputs: number of (conditional) inputs
        :param n_outputs: number of outputs
        :param n_hiddens: list with number of hidden units for each hidden layer
        :param act_fun: name of activation function
        :param n_layers: number of layers in the flow
        :param n_comps: number of gaussians for each conditional of target made
        :param batch_norm: whether to use batch normalization between mades in the flow
        :param output_order: order of outputs of last made
        :param mode: strategy for assigning degrees to hidden nodes: can be 'random' or 'sequential'
        :param input: theano variable to serve as input; if None, a new variable is created
        :param output: theano variable to serve as output; if None, a new variable is created
        """

        # save input arguments
        self.n_inputs = n_inputs
        self.n_outputs = n_outputs
        self.n_hiddens = n_hiddens
        self.act_fun = act_fun
        self.n_layers = n_layers
        self.n_comps = n_comps
        self.batch_norm = batch_norm
        self.mode = mode

        self.input = tt.matrix('x', dtype=dtype) if input is None else input
        self.y = tt.matrix('y', dtype=dtype) if output is None else output
        self.parms = []

        # maf
        self.maf = ConditionalMaskedAutoregressiveFlow(n_inputs, n_outputs, n_hiddens, act_fun, n_layers, batch_norm, output_order, mode, self.input, self.y, rng)
        self.bns = self.maf.bns
        self.parms += self.maf.parms
        self.output_order = self.maf.output_order

        # mog made
        output_order = output_order if output_order == 'random' else self.maf.mades[-1].output_order[::-1]
        self.made = mades.ConditionalMixtureOfGaussiansMade(n_inputs, n_outputs, n_hiddens, act_fun, n_comps, output_order, mode, self.input, self.maf.u, rng)
        self.parms += self.made.parms

        # log likelihoods
        self.L = self.made.L + self.maf.logdet_dudy
        self.L.name = 'L'

        # train objective
        self.trn_loss = -tt.mean(self.L)
        self.trn_loss.name = 'trn_loss'

        # theano evaluation functions, will be compiled when first needed
        self.eval_lprob_f = None
        self.eval_grad_f = None
        self.eval_score_f = None
        self.eval_us_f = None

    def reset_theano_functions(self):
        """
        Resets theano functions, so that they are compiled again when needed.
        """

        self.eval_lprob_f = None
        self.eval_grad_f = None
        self.eval_score_f = None
        self.eval_us_f = None

        self.maf.reset_theano_functions()
        self.made.reset_theano_functions()

        for bn in self.bns:
            bn.reset_theano_functions()

    def eval(self, xy, log=True):
        """
        Evaluate log probabilities for given input-output pairs.
        :param xy: a pair (x, y) where x rows are inputs and y rows are outputs
        :param log: whether to return probabilities in the log domain
        :return: log probabilities: log p(y|x)
        """

        # compile theano function, if haven't already done so
        if self.eval_lprob_f is None:
            self.eval_lprob_f = theano.function(
                inputs=[self.input, self.y],
                outputs=self.L,
                givens=[(bn.m, bn.bm) for bn in self.bns] + [(bn.v, bn.bv) for bn in self.bns]
            )

        x, y, one_datapoint = util.misc.prepare_cond_input(xy, dtype)

        lprob = self.eval_lprob_f(x, y)
        lprob = lprob[0] if one_datapoint else lprob

        return lprob if log else np.exp(lprob)

    def grad_log_p(self, xy):
        """
        Evaluate the gradient of the log probability wrt the output, for given input-output pairs.
        :param xy: a pair (x, y) where x rows are inputs and y rows are outputs
        :return: gradient d/dy log p(y|x)
        """

        # compile theano function, if haven't already done so
        if getattr(self, 'eval_grad_f', None) is None:
            self.eval_grad_f = theano.function(
                inputs=[self.input, self.y],
                outputs=tt.grad(tt.sum(self.L), self.y),
                givens=[(bn.m, bn.bm) for bn in self.bns] + [(bn.v, bn.bv) for bn in self.bns]
            )

        x, y, one_datapoint = util.misc.prepare_cond_input(xy, dtype)

        grad = self.eval_grad_f(x, y)
        grad = grad[0] if one_datapoint else grad

        return grad

    def score(self, xy):
        """
        Evaluate the gradient of the log probability wrt the input, for given input-output pairs.
        :param xy: a pair (x, y) where x rows are inputs and y rows are outputs
        :return: gradient d/dx log p(y|x)
        """

        # compile theano function, if haven't already done so
        if self.eval_score_f is None:
            self.eval_score_f = theano.function(
                inputs=[self.input, self.y],
                outputs=tt.grad(tt.sum(self.L), self.input),
                givens=[(bn.m, bn.bm) for bn in self.bns] + [(bn.v, bn.bv) for bn in self.bns]
            )

        x, y, one_datapoint = util.misc.prepare_cond_input(xy, dtype)

        grads = self.eval_score_f(x, y)
        grads = grads[0] if one_datapoint else grads

        return grads

    def gen(self, x, n_samples=None, u=None, rng=np.random):
        """
        Generate samples, by propagating random numbers through each made, after conditioning on input x.
        :param x: input vector
        :param n_samples: number of samples, 1 if None
        :param u: random numbers to use in generating samples; if None, new random numbers are drawn
        :return: samples
        """

        if n_samples is None:
            return self.gen(x, 1, u if u is None else u[np.newaxis, :], rng)[0]

        y = rng.randn(n_samples, self.n_outputs).astype(dtype) if u is None else u
        y = self.made.gen(x, n_samples, y, rng)
        y = self.maf.gen(x, n_samples, y, rng)

        return y

    def calc_random_numbers(self, xy):
        """
        Givan a dataset, calculate the random numbers used internally to generate the dataset.
        :param xy: a pair (x, y) of numpy arrays, where x rows are inputs and y rows are outputs
        :return: numpy array, rows are corresponding random numbers
        """

        # compile theano function, if haven't already done so
        if self.eval_us_f is None:
            self.eval_us_f = theano.function(
                inputs=[self.input, self.y],
                outputs=self.made.u
            )

        x, y, one_datapoint = util.misc.prepare_cond_input(xy, dtype)
        u = self.eval_us_f(x, y)

        return u[0] if one_datapoint else u


class MaskedAutoregressiveFlow_SVI:
    """
    Implements the SVI version of a Masked Autoregressive Flow.
    """

    def __init__(self, n_inputs, n_hiddens, act_fun, n_mades, input_order='sequential', mode='sequential', input=None, rng=np.random):
        """
        Constructor.
        :param n_inputs: number of inputs
        :param n_hiddens: list with number of hidden units for each hidden layer
        :param act_fun: name of activation function
        :param n_mades: number of mades in the flow
        :param input_order: order of inputs of last made
        :param mode: strategy for assigning degrees to hidden nodes: can be 'random' or 'sequential'
        :param input: theano variable to serve as input; if None, a new variable is created
        """

        # save input arguments
        self.n_inputs = n_inputs
        self.n_hiddens = n_hiddens
        self.act_fun = act_fun
        self.n_mades = n_mades
        self.mode = mode

        self.input = tt.matrix('x', dtype=dtype) if input is None else input
        self.mps = []
        self.sps = []
        self.all_us = []

        self.mades = []
        self.u = self.input

        for i in xrange(n_mades):

            # create a new made
            made = mades.GaussianMade_SVI(n_inputs, n_hiddens, act_fun, input_order, mode, self.u, rng)
            self.mades.append(made)
            self.mps += made.mps
            self.sps += made.sps
            self.all_us += made.all_us
            input_order = input_order if input_order == 'random' else made.input_order[::-1]

            # inverse autoregressive transform
            self.u = made.u
            self.u.name = 'u' + str(n_mades - i - 1)

        self.input_order = self.mades[0].input_order
        self.parms = self.mps + self.sps

        # the sum of logp's is equal to the sum of logdet's
        sum_logp = sum([tt.sum(made.logp, axis=1) for made in self.mades])
        sum_logp.name = 'sum_logp'

        # log likelihoods
        self.L = -0.5 * (n_inputs * np.log(2 * np.pi) - sum_logp + tt.sum(self.u ** 2, axis=1))
        self.L.name = 'L'

        # train objective
        self.trn_loss = -tt.mean(self.L)
        self.trn_loss.name = 'trn_loss'

        # theano evaluation functions, will be compiled when first needed
        self.eval_lprob_f = None
        self.eval_lprob_f_rand = None
        self.eval_lprob_f_rand_const = None

    def reset_theano_functions(self):
        """
        Resets theano functions, so that they are compiled again when needed.
        """

        self.eval_lprob_f = None
        self.eval_lprob_f_rand = None
        self.eval_lprob_f_rand_const = None

        for made in self.mades:
            made.reset_theano_functions()

    def _create_constant_noise_across_datapoints(self, n_data):
        """
        Helper function. Creates and returns new theano variables representing noise, where noise is the same across
        datapoints in the minibatch. Useful for binding the original noise variables in an evaluation function where
        randomness is required but same predictions are needed across minibatch.
        """

        all_us = []

        for made in self.mades:
            all_us += made._create_constant_noise_across_datapoints(n_data)

        return all_us

    def _create_zero_noise(self, n_data):
        """
        Helper function. Creates and returns new theano variables representing zero noise. Useful for binding the
        original noise variables in an evaluation function where randomness is not required.
        """

        all_us = []

        for made in self.mades:
            all_us += made._create_zero_noise(n_data)

        return all_us

    def eval(self, x, log=True, rand=False, const_noise=False):
        """
        Evaluate log probabilities for given inputs.
        :param x: data matrix where rows are inputs
        :param log: whether to return probabilities in the log domain
        :param rand: whether to inject randomness to the activations
        :param const_noise: whether the injected randomness is the same across datapoints
        :return: list of log probabilities log p(x)
        """

        x = np.asarray(x, dtype=dtype)
        one_datapoint = x.ndim == 1
        x = x[np.newaxis, :] if one_datapoint else x

        if rand:

            if const_noise:

                # compile theano function, if haven't already done so
                if self.eval_lprob_f_rand_const is None:

                    n_data = tt.iscalar('n_data')
                    all_us = self._create_constant_noise_across_datapoints(n_data)

                    self.eval_lprob_f_rand_const = theano.function(
                        inputs=[self.input, n_data],
                        outputs=self.L,
                        givens=zip(self.all_us, all_us)
                    )

                lprob = self.eval_lprob_f_rand_const(x, x.shape[0])

            else:

                # compile theano function, if haven't already done so
                if self.eval_lprob_f_rand is None:
                    self.eval_lprob_f_rand = theano.function(
                        inputs=[self.input],
                        outputs=self.L
                    )

                lprob = self.eval_lprob_f_rand(x)

        else:

            # compile theano function, if haven't already done so
            if self.eval_lprob_f is None:

                n_data = tt.iscalar('n_data')
                all_us = self._create_zero_noise(n_data)

                self.eval_lprob_f = theano.function(
                    inputs=[self.input, n_data],
                    outputs=self.L,
                    givens=zip(self.all_us, all_us)
                )

            lprob = self.eval_lprob_f(x, x.shape[0])

        lprob = lprob[0] if one_datapoint else lprob

        return lprob if log else np.exp(lprob)

    def gen(self, n_samples=None, rand=False, const_noise=False, u=None, rng=np.random):
        """
        Generate samples, by propagating random numbers through each made.
        :param n_samples: number of samples, 1 if None
        :param rand: whether to inject randomness to the activations
        :param const_noise: whether the injected randomness is the same across samples
        :param u: random numbers to use in generating samples; if None, new random numbers are drawn
        :return: samples
        """

        if n_samples is None:
            return self.gen(1, rand, const_noise, u if u is None else u[np.newaxis, :], rng)[0]

        x = rng.randn(n_samples, self.n_inputs).astype(dtype) if u is None else u

        for made in self.mades[::-1]:
            x = made.gen(n_samples, rand, const_noise, x, rng)

        return x

    def calc_random_numbers(self, x):
        raise NotImplementedError()
